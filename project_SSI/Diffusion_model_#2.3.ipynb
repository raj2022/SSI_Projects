{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13df500c",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](PASTE_YOUR_COLAB_LINK_HERE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8902cd1f",
   "metadata": {},
   "source": [
    "#### Diffusion Model \n",
    "Diffusion model with variational Autoencoder. With the help of data augmentation, increasing the number of trainable datapoint.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b9a74",
   "metadata": {},
   "source": [
    "This notebook contains the same diffusion model as of [this one](https://github.com/raj2022/SSI_Projects/blob/main/project_SSI/Diffusion_model_%232.1.ipynb) and [this one as well](https://github.com/raj2022/SSI_Projects/blob/main/project_SSI/Diffusion_model_%232.ipynb) with a bit change in the architecture. \n",
    "\n",
    "* Added a few more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # To use the CPU only\n",
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Use CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e70b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "## Import Required Libraries for data Generator \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83fc02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clear out the gpu memory\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e775c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list out all the number of GPUs available on the systtem\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32d440de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data (replace with your own data loading)\n",
    "fileIN = '../jet_notebooks/Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5'\n",
    "f = h5py.File(fileIN)\n",
    "# Load and preprocess data (replace with your data loading)\n",
    "# Assuming you have jet images in a numpy array 'jet_images'\n",
    "# Normalize the pixel values between 0 and 1\n",
    "jet_images = np.array(f.get('jetImage')).astype('float32') / 255.0\n",
    "dataset_shape = jet_images.shape\n",
    "image_size = dataset_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3084f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "# Create Data Generators:\n",
    "# Define data augmentation settings\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range=90,      # Changed from 20 to 90\n",
    "    width_shift_range=0.1,  # Random horizontal shift by 10% of image width\n",
    "    height_shift_range=0.1, # Random vertical shift by 10% of image height\n",
    "    shear_range=0.2,        # Random shear transformation\n",
    "    zoom_range=0.2,         # Random zoom between 80% and 120% of original size\n",
    "    horizontal_flip=True,   # Randomly flip images horizontally\n",
    "    fill_mode='nearest'     # Fill newly created pixels with nearest neighbors\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f83803bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the images\n",
    "noise_factor = 0.6   # Changed from 0.5 \n",
    "noisy_jet_images = jet_images + noise_factor * np.random.normal(size=jet_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094f51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the jet images dataset to match the model's input shape\n",
    "jet_images_reshaped = jet_images.reshape((-1, image_size, image_size, 1))\n",
    "noisy_jet_images_reshaped = noisy_jet_images.reshape((-1, image_size, image_size, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa78fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators for noisy and clean images\n",
    "batch_size = 32\n",
    "train_noisy_datagen = data_augmentation.flow(noisy_jet_images_reshaped, batch_size=batch_size, shuffle=True)\n",
    "train_clean_datagen = data_augmentation.flow(jet_images_reshaped, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac57e18",
   "metadata": {},
   "source": [
    "###  Issues:\n",
    "    1. Validation loss falls sharply while the training loss falls smoothly. \n",
    "---\n",
    "    -> Trying to reduce the model complexity\n",
    "    2. There are another methods which we can implement:\n",
    "        1. Reduce Model Complexity: \n",
    "        2. Regularization Techniques\n",
    "        3. Data Augmentation\n",
    "        4. Early Stopping\n",
    "        5. Validation Data\n",
    "        6. Learning Rate Schedule\n",
    "        7. Different Architecture\n",
    "        8. More Datas\n",
    "        9. Batch Normalization\n",
    "        10. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "097ed012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /cvmfs/sft.cern.ch/lcg/views/LCG_103cuda/x86_64-centos9-gcc11-opt/lib/python3.9/site-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Define the autoencoder architecture\n",
    "input_img = Input(shape=(image_size, image_size, 1))\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(input_img)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# Extra convolutional layer\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edcdaae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8263ad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 100, 100, 16)      160       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 100, 100, 16)     64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 50, 50, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50, 50, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 50, 50, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 50, 50, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 25, 25, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 25, 25, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 25, 25, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 25, 25, 32)        18464     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 25, 25, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 50, 50, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 50, 50, 16)        4624      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 50, 50, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 100, 100, 16)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 100, 100, 1)       145       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,169\n",
      "Trainable params: 46,849\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78825f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train the autoencoder\n",
    "# history = autoencoder.fit(noisy_jet_images_reshaped, jet_images_reshaped,\n",
    "#                           epochs=5,\n",
    "#                           batch_size=64,\n",
    "#                           shuffle=True,\n",
    "#                           validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d11663e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Early Stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee5bf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the jet images dataset to match the model's input shape\n",
    "jet_images_reshaped = jet_images.reshape((-1, image_size, image_size, 1))\n",
    "noisy_jet_images_reshaped = noisy_jet_images.reshape((-1, image_size, image_size, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf3013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/sraj/ipykernel_441540/2041531694.py:2: UserWarning: `model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = autoencoder.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - ETA: 0s - batch: 155.5000 - size: 32.0000 - loss: 15.6342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_103cuda/x86_64-centos9-gcc11-opt/lib/python3.9/site-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 27s 63ms/step - batch: 155.5000 - size: 32.0000 - loss: 15.6342 - val_loss: 9.7067\n",
      "Epoch 2/50\n",
      "312/312 [==============================] - 19s 62ms/step - batch: 155.5000 - size: 31.9487 - loss: 5.7970 - val_loss: 2.5613\n",
      "Epoch 3/50\n",
      "312/312 [==============================] - 19s 62ms/step - batch: 155.5000 - size: 31.9487 - loss: 1.1583 - val_loss: 0.3661\n",
      "Epoch 4/50\n",
      "312/312 [==============================] - 19s 62ms/step - batch: 155.5000 - size: 31.9487 - loss: 0.1657 - val_loss: 0.0470\n",
      "Epoch 5/50\n",
      "312/312 [==============================] - 20s 64ms/step - batch: 155.5000 - size: 31.9487 - loss: 0.0345 - val_loss: 0.0216\n",
      "Epoch 6/50\n",
      "312/312 [==============================] - 19s 62ms/step - batch: 155.5000 - size: 31.9487 - loss: 0.0157 - val_loss: 0.0113\n",
      "Epoch 7/50\n",
      "312/312 [==============================] - 19s 62ms/step - batch: 155.5000 - size: 31.9487 - loss: 0.0089 - val_loss: 0.0079\n",
      "Epoch 8/50\n",
      "312/312 [==============================] - 19s 62ms/step - batch: 155.5000 - size: 31.9487 - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 9/50\n",
      "312/312 [==============================] - 19s 62ms/step - batch: 155.5000 - size: 31.9487 - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 10/50\n",
      "312/312 [==============================] - 19s 62ms/step - batch: 155.5000 - size: 31.9487 - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 11/50\n",
      "312/312 [==============================] - 19s 62ms/step - batch: 155.5000 - size: 31.9487 - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 12/50\n",
      "312/312 [==============================] - 19s 61ms/step - batch: 155.5000 - size: 31.9487 - loss: 0.0057 - val_loss: 0.0059\n",
      "Epoch 13/50\n",
      "312/312 [==============================] - 19s 62ms/step - batch: 155.5000 - size: 31.9487 - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 14/50\n",
      "312/312 [==============================] - ETA: 0s - batch: 155.5000 - size: 31.9487 - loss: 0.0055"
     ]
    }
   ],
   "source": [
    "# Training the autoencoder with the genrated data or the augmented data\n",
    "history = autoencoder.fit_generator(\n",
    "    zip(train_noisy_datagen, train_clean_datagen),\n",
    "    steps_per_epoch=len(noisy_jet_images_reshaped) // batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=(noisy_jet_images_reshaped, jet_images_reshaped), \n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0920e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss of Denoising Autoencoder')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Denoise some sample images\n",
    "num_samples = 5\n",
    "sample_indices = np.random.randint(0, len(noisy_jet_images_reshaped), num_samples)\n",
    "sample_noisy_images = noisy_jet_images_reshaped[sample_indices]\n",
    "sample_denoised_images = autoencoder.predict(sample_noisy_images)\n",
    "\n",
    "# Display original, noisy, and denoised images\n",
    "for i in range(num_samples):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(jet_images_reshaped[sample_indices[i]].reshape((image_size, image_size)), cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    \n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(noisy_jet_images_reshaped[sample_indices[i]].reshape((image_size, image_size)), cmap='gray')\n",
    "    plt.title('Noisy Image')\n",
    "    \n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(sample_denoised_images[i].reshape((image_size, image_size)), cmap='gray')\n",
    "    plt.title('Denoised Image')\n",
    "    \n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(sample_denoised_images[i].reshape((image_size, image_size)) - \n",
    "               jet_images_reshaped[sample_indices[i]].reshape((image_size, image_size)), cmap='gray')\n",
    "    plt.title('Difference')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71486f22",
   "metadata": {},
   "source": [
    "Adding colorbar to the above image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdbd434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Denoise some sample images. \n",
    "num_samples = 5\n",
    "sample_indices = np.random.randint(0, len(noisy_jet_images_reshaped), num_samples)\n",
    "sample_noisy_images = noisy_jet_images_reshaped[sample_indices]\n",
    "sample_denoised_images = autoencoder.predict(sample_noisy_images)\n",
    "\n",
    "# Display original, noisy, denoised images, and difference with custom colormap\n",
    "for i in range(num_samples):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(jet_images_reshaped[sample_indices[i]].reshape((image_size, image_size)), cmap='Blues')\n",
    "    plt.title('Original Image')\n",
    "    plt.colorbar()\n",
    "    plt.xlim(10,90)\n",
    "    plt.ylim(10,90)\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(noisy_jet_images_reshaped[sample_indices[i]].reshape((image_size, image_size)), cmap='Blues')\n",
    "    plt.title('Noisy Image')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(sample_denoised_images[i].reshape((image_size, image_size)), cmap='Blues')\n",
    "    plt.title('Denoised Image')\n",
    "    plt.colorbar()\n",
    "    plt.xlim(10,90)\n",
    "    plt.ylim(10,90)\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    # Calculate the difference between original and denoised images\n",
    "    difference = jet_images_reshaped[sample_indices[i]] - sample_denoised_images[i].reshape((image_size, image_size))\n",
    "    # Display a specific slice of the difference array (e.g., the middle slice)\n",
    "    middle_slice = difference[:,:,difference.shape[2]//2]\n",
    "    plt.imshow(middle_slice, cmap='seismic', vmin=-1, vmax=1)  # Customizing the colormap and range\n",
    "    plt.title('Difference')\n",
    "    plt.colorbar()\n",
    "    plt.xlim(10,90)\n",
    "    plt.ylim(10,90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f46f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
